# Summary of Machine Learning Algorithm

kNN(k-近邻算法)  
原理：存在一个样本数据集合，也称作训练样本集，并且样本集中每一个数据都存在标签，即我们知道样本集中每一数据和所属分类之间的对应关系。输入没有标签的新数据后，将新数据的每一个特征和样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，k个最相似数据中出现最多的分类，作为新数据的预测分类。  
pros and cons：（1）kNN是分类数据最简单的算法，（2）但是kNN必须保存全部数据集，如果训练数据集很大，必须使用很大的存储空间。（3）此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。（4）kNN的另一个缺陷就是，他无法给出任何数据的基础结构信息，因此我们无法知道平均实例样本和典型实例样本具有什么特征。  
是否存在一种算法减少存储空间和计算开销？k决策树就是kNN的优化版本，可以节省大量的计算开销.  
  
  


